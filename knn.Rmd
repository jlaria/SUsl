---
title: "Practice time!"
output:
  html_document:
    highlight: kate
    theme: lumen
    toc: no
    toc_float: no
  pdf_document:
    toc: no
header-includes: \usepackage{xcolor}
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, collapse = T, cache = T, fig.width = 4, fig.height = 3, fig.align = "center")
```

# k Nearest Neighbor Classifiers

## A simple 2D example

File `tumor.RData` contains two simulated data frames `df` (for training) and `df_test` for test. The response variable `recovery` describes the outcome of the treatment for patients with features `age` and `tumor_size`.

We start loading the data in R.

```{r}
load("source/tumor.RData")
```

We will use the implementation of kNN in function `knn` from `class` library.

```{r}
library(class)
```

First, we will prepare the data.

```{r}
# We separate the response from the data.
cl = df$recovery
df$recovery = NULL

cl_test = df_test$recovery
df_test$recovery = NULL

# Scale the training data
X = scale(df)

# Scale the test data
X_test = scale(df_test, 
               center = attr(X, "scaled:center"), 
               scale = attr(X, "scaled:scale")) 

# Predict the test data using the training
# kNN with k=5 neighbors
pred = knn(X, X_test, cl, k = 5)
```

In this case, we can plot the kNN decision boundary, because there are only two explanatory variables. In higher dimensions, this is impossible.

```{r}
library(ggplot2)
source("https://raw.githubusercontent.com/jlaria/dsclass/master/R/knn_viz.R")

# The training data
knn_viz(X, cl, k=5, prob_size = F)

# The test data
knn_viz(X, cl, k = 5, validate = X_test, cl_validate = cl_test, prob_size = F)
```

In the next section, we are going to discuss how to interpret the predictions, and evaluate the performance of the algorithm using the test data. 

## Evaluating the performance of classifiers

Binary classifiers, such as kNN, can make two types of errors: 

+ it can incorrectly assign an individual who is **\textcolor{red}{red}** to the *\textcolor{cyan}{blue}* category.
+ (the opposite) it can incorrectly assign an individual who is **\textcolor{cyan}{blue}** to the *\textcolor{red}{red}* category.

### Confusion matrix

The confusion matrix is an important tool to verify a model's accuracy.

```{r}
# Confusion matrix (Test data)
t = table(pred = pred, real = cl_test); t
```

Based on this confusion matrix, we define the following measures.

+ True positive (TP) 
```{r}
t[2,2]
```
+ True negative (TN)
```{r}
t[1,1]
```
+ False positive (FP)
```{r}
t[2,1]
```
+ False negative (FN)
```{r}
t[1,2]
```
+ Positive (P) $= TP + FN$
```{r}
sum(t[,2])
```
+ Negative (N) $= TN + FP$
```{r}
sum(t[,1])
```
+ True positive rate (TPR) or sensitivity $=TP/P$
```{r}
t[2,2]/sum(t[,2])
```
+ True negative rate (TNR) or specificity $= TN/N$
```{r}
t[1,1]/sum(t[,1])
```
+ False positive rate (FPR) $=FP/N$
```{r}
t[2,1]/sum(t[,1])
```
+ False negative rate (FNR) $=FN/P$
```{r}
t[1,2]/sum(t[,2])
```
+ Correct classification rate (CCR) or Accuracy (ACC) $=(TP+TN)/(P+N)$
```{r}
sum(diag(t))/sum(t)
```

### Cross-validation

In our kNN example, we have 40 observations in the training data `df`, somewhat representing our present knowledge. Those in `df_test` are merely illustrative, and they represent the future. We can't use them here to draw conclussions, because in a real scenario they would be unknown. Luckly, we can obtain a decent approximation of the future error of our classification models from the train data.

An intuitive approach would be *cross-validation*. 

1. We randomly split the data `X` into two data sets (training and validation)
```{r}
train.idx = sample(nrow(df), 20)
X_train = X[train.idx, ]
X_validate = X[-train.idx, ]
 
cl_train = cl[train.idx]
cl_validate = cl[-train.idx]
```

2. The data in `X_train` is used to train the model, whereas `X_validate` is used to evaluate its performance, or select appropriate values for the hyper-parameters. For instance, $k$ is an hyper-parameter of the kNN algorithm.
```{r}
pred_validate = knn(X_train, X_validate, cl_train, k = 5) 
t = table(pred = pred_validate, real = cl_validate); t

# Correct classification rate
sum(diag(t))/sum(t)
```

In this case, we have used a $50\%-50\%$ split, but other choices are possible. This is one of the simplest cross-validation schemes, but there are more complicated approaches.
We could repeat steps 1 - 2 many times and estimate errors using the averages over all possible partitions.

We can use the function `prop.split` to automatically split the data of length `N` using those proportions given by `p`.
Now, we can split the data at random, and repeat this process many times.

```{r}
source("https://raw.githubusercontent.com/jlaria/dsclass/master/R/knn_viz.R")
# Multiple cross-validation

nruns = 30
ccr = rep(0, nruns)
for (run in 1:nruns) {
  
  # Compute train/validate indices
  idx = prop.split(nrow(X), c(0.5, 0.5))
  
  # Split the data
  X_train = X[ idx[[1]] , ]
  cl_train = cl[ idx[[1]] ]
  
  X_validate = X[ idx[[2]], ]
  cl_validate = cl[ idx[[2]] ]
  
  # Validate the model
  pred_validate = knn(X_train, X_validate, cl_train, k = 5)
  t = table(pred_validate, cl_validate)
  
  # Save the CCR
  ccr[run] = sum(diag(t))/sum(t)
}

# Overall average CCR
mean(ccr)

```

Notice anything? The overall correct classification rate estimated using this method almost equals the CCR computed in the previous section. This means that we have estimated the future accuracy of kNN using only those 40 observations in the training sample.

### $k$-fold cross validation

Another approach to estimate this error is $k$-fold cross-validation.

1. We first split the data into $k$ (approximately) equally sized folds
```{r}
# 10-fold cross validation
idx = prop.split(40, rep(0.1, 10))
```

2. In each step, leave one of the folds to validate, and use the remaining folds as training data.

```{r}
ccr = rep(0, 10)

for (fold in 1:10) {
  # split the data, leaving one of the folds to validate
  X_validate = X[ idx[[fold]], ]
  cl_validate = cl[ idx[[fold]] ]
  
  X_train = X[ -idx[[fold]], ]
  cl_train = cl[ -idx[[fold]] ]
  
  # Validate the model
  pred_validate = knn(X_train, X_validate, cl_train, k = 5)
  t = table(pred_validate, cl_validate)
  
  # Save the CCR
  ccr[fold] = sum(diag(t))/sum(t)
}

# 10-fold CV ccr
mean(ccr)
```

Most common choices for $k$ are 2, 5, 10, and the sample size. This last choice is commonly referred to as leave-one-out cross validation.

<!-- ## Example: Credit -->

<!-- ```{r, eval=FALSE, echo=FALSE} -->
<!-- library(ISLR) -->
<!-- df = Default -->

<!-- train.idx = sample(10000, 5000) -->
<!-- df = Default[train.idx, ] -->
<!-- df_test = Default[-train.idx,] -->
<!-- save(df, df_test, file="source/Default.RData") -->
<!-- ``` -->

<!-- In this section, we are going to work with a larger data set, containing information on five thousand customers. The aim here is to predict which customers will default on their credit card debt. -->

<!-- ```{r} -->
<!-- load("source/Default.RData") -->
<!-- ``` -->

<!-- The training data is `df`. We will use `df_test` for illustration here, but in a realistic scenario `df_test` would be missing. -->

<!-- A first approximation to the problem is to split the data `df` in two sets: `df_train` and `df_validate`.   -->

<!-- ```{r} -->
<!-- # Transform the data -->
<!-- cl = df$default -->
<!-- X = model.matrix(default~., df)[,-1] -->
<!-- X[,c(2,3)] = scale(X[,c(2,3)]) -->

<!-- # Split the data in two sets -->
<!-- set.seed(1) -->
<!-- idx = prop.split(nrow(df), c(0.5, 0.5)) -->

<!-- # Create the train/validate matrices -->
<!-- cl_train = cl[idx[[1]]] -->
<!-- X_train = X[idx[[1]],] -->
<!-- cl_validate = cl[idx[[2]]] -->
<!-- X_validate = X[idx[[2]],] -->

<!-- # predict -->
<!-- pred = knn(train = X_train, test = X_validate, cl = cl_train, k=15, prob = T) -->

<!-- probs = attr(pred, "prob") -->
<!-- pred = ifelse(prob > 0.95, "No", "Yes") -->

<!-- t = table(pred. = pred, real. = cl_validate); t -->
<!-- ccr = sum(diag(t))/sum(t); ccr -->
<!-- mean(cl_validate=="No") -->



<!-- ``` -->


